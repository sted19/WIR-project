{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import threading\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.path.join(os.getcwd(),'Datasets')\n",
    "\n",
    "movies_path = os.path.join(datasets,'movies_dataset')\n",
    "\n",
    "user_based_dict_path = os.path.join(movies_path,'utility_matrix_user_based.txt')\n",
    "item_based_dict_path = os.path.join(movies_path,'utility_matrix_item_based.txt')\n",
    "metadata_path = os.path.join(movies_path,'movies_metadata.txt')\n",
    "\n",
    "\n",
    "CORES = multiprocessing.cpu_count()\n",
    "cliques = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metadata file, with topic informations ecc\n",
    "with open(metadata_path,'r') as fr:\n",
    "    movies_metadata = json.load(fr)\n",
    "\n",
    "\n",
    "def convert_metadata_to_int(movies_metadata):\n",
    "    movies_metadata_int = {}\n",
    "\n",
    "    for key1 in movies_metadata.keys():\n",
    "        key11 = int(key1)\n",
    "\n",
    "        movies_metadata_int[key11] = movies_metadata[key1]\n",
    "    \n",
    "    return movies_metadata_int\n",
    "\n",
    "movies_metadata = convert_metadata_to_int(movies_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nwith open(item_based_dict_path,'r') as fr:\\n    item_based_utility_movies = json.load(fr)\\n\""
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# load user_based dictionary dataset\n",
    "with open(user_based_dict_path,'r') as fr:\n",
    "    user_based_utility_matrix = json.load(fr)\n",
    "'''\n",
    "with open(item_based_dict_path,'r') as fr:\n",
    "    item_based_utility_movies = json.load(fr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def convert_ratings_to_float(user_based_utility):\n",
    "    user_based_correct = {}\n",
    "\n",
    "    for key1 in user_based_utility.keys():\n",
    "        for key2 in user_based_utility[key1].keys():\n",
    "\n",
    "            key11 = int(float(key1))\n",
    "            key22 = int(float(key2))\n",
    "\n",
    "            if(user_based_correct.get(key11) == None):\n",
    "                user_based_correct[key11] = {}\n",
    "            \n",
    "            user_based_correct[key11][key22] = float(user_based_utility[key1][key2])\n",
    "    \n",
    "    return user_based_correct\n",
    "\n",
    "user_based_utility_matrix = convert_ratings_to_float(user_based_utility_matrix)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 155968 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 2851 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 769 has no metadata, eliminating\nitem 26649 has no metadata, eliminating\nitem 79299 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 73370 has no metadata, eliminating\nitem 40697 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 69849 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 122926 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 5069 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 1434 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 69849 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 26649 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 100044 has no metadata, eliminating\nitem 100450 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 73370 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 107780 has no metadata, eliminating\nitem 26379 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 27049 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 77328 has no metadata, eliminating\nitem 77330 has no metadata, eliminating\nitem 77359 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 42987 has no metadata, eliminating\nitem 103277 has no metadata, eliminating\nitem 27724 has no metadata, eliminating\nitem 87308 has no metadata, eliminating\nitem 107718 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 152284 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 155968 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 81731 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 73759 has no metadata, eliminating\nitem 32352 has no metadata, eliminating\nitem 55207 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 96075 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 40697 has no metadata, eliminating\nitem 87308 has no metadata, eliminating\nitem 31193 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 769 has no metadata, eliminating\nitem 791 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 73759 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 5069 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 1434 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 2270 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 93988 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 107780 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 2851 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 55207 has no metadata, eliminating\nitem 77854 has no metadata, eliminating\nitem 31193 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 38198 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 5069 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 26649 has no metadata, eliminating\nitem 69849 has no metadata, eliminating\nitem 93988 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27708 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 5069 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 81731 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 730 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 122926 has no metadata, eliminating\nitem 108977 has no metadata, eliminating\nitem 31193 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 99766 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27049 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 90945 has no metadata, eliminating\nitem 155968 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 100044 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 69849 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 27049 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 62336 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 4568 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 81731 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 1421 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 27049 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 2851 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 40697 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 108727 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 38198 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 77328 has no metadata, eliminating\nitem 77359 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 155968 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 31193 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 93988 has no metadata, eliminating\nitem 90945 has no metadata, eliminating\nitem 96075 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 69849 has no metadata, eliminating\nitem 4568 has no metadata, eliminating\nitem 152284 has no metadata, eliminating\nitem 169906 has no metadata, eliminating\nitem 73370 has no metadata, eliminating\nitem 26649 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 27049 has no metadata, eliminating\nitem 77328 has no metadata, eliminating\nitem 77330 has no metadata, eliminating\nitem 77359 has no metadata, eliminating\nitem 103277 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 81731 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 1142 has no metadata, eliminating\nitem 2851 has no metadata, eliminating\nitem 31193 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 5069 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4568 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 77328 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 31193 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 26693 has no metadata, eliminating\nitem 100044 has no metadata, eliminating\nitem 150548 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 100538 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 77328 has no metadata, eliminating\nitem 77359 has no metadata, eliminating\nitem 26587 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 5069 has no metadata, eliminating\nitem 26379 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 4207 has no metadata, eliminating\nitem 1421 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 7669 has no metadata, eliminating\nitem 27611 has no metadata, eliminating\nitem 52281 has no metadata, eliminating\nitem 106642 has no metadata, eliminating\nitem 155968 has no metadata, eliminating\nitem 90647 has no metadata, eliminating\nitem 720 has no metadata, eliminating\nitem 100044 has no metadata, eliminating\nitem 26649 has no metadata, eliminating\nitem 85780 has no metadata, eliminating\nitem 139195 has no metadata, eliminating\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([   142,    720,    730,    769,    770,    791,    821,   1107,\n         1122,   1133,   1142,   1166,   1316,   1421,   1434,   1709,\n         2258,   2270,   2851,   4051,   4207,   4568,   5069,   5209,\n         5738,   6955,   7669,  26379,  26587,  26649,  26693,  27049,\n        27611,  27708,  27724,  30991,  31193,  32352,  32600,  38198,\n        40697,  42987,  52281,  55207,  62336,  69849,  70828,  73370,\n        73759,  77328,  77330,  77359,  77854,  79299,  81731,  85780,\n        87073,  87308,  90647,  90945,  93988,  96062,  96075,  99766,\n       100044, 100058, 100089, 100450, 100538, 103277, 104155, 106334,\n       106642, 107718, 107780, 108727, 108977, 118392, 119565, 122926,\n       126106, 139195, 143351, 144050, 147250, 150548, 151763, 152284,\n       155968, 163921, 167570, 169906, 170745, 171749, 173535])"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "removed = []\n",
    "\n",
    "def remove_items_not_in_metadata(user_based_utility, movies_metadata):\n",
    "    for key1 in user_based_utility.keys():\n",
    "        dict_1 = user_based_utility[key1]\n",
    "        keys = list(dict_1.keys()).copy()\n",
    "        for idx in range(len(keys)):\n",
    "            key2 = keys[idx]\n",
    "            if movies_metadata.get(key2) == None:\n",
    "                print('item {} has no metadata, eliminating'.format(key2))\n",
    "                removed.append(key2)\n",
    "                del dict_1[key2]\n",
    "    return user_based_utility, removed\n",
    "\n",
    "user_based_utility_matrix, removed = remove_items_not_in_metadata(user_based_utility_matrix, movies_metadata)\n",
    "removed_unique = np.unique(removed)\n",
    "removed_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# functions to operate division in test/train dictionaries\n",
    "\n",
    "def divide_dataset(dataset):\n",
    "    folds = [{},{},{},{}]\n",
    "\n",
    "    for key1 in dataset.keys():\n",
    "        for key2 in dataset[key1].keys():\n",
    "            rand_index = np.random.randint(4)\n",
    "            \n",
    "            tmp_dict = folds[rand_index]\n",
    "\n",
    "            if(tmp_dict.get(key1) == None):\n",
    "                tmp_dict[key1] = {}\n",
    "            \n",
    "            tmp_dict[key1][key2] = dataset[key1][key2]\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def merge_dicts(dicts):\n",
    "    ret = {}\n",
    "\n",
    "    for i in range(len(dicts)):\n",
    "        tmp_dict = dicts[i]\n",
    "        for key1 in tmp_dict.keys():\n",
    "            for key2 in tmp_dict[key1].keys():\n",
    "\n",
    "                if(ret.get(key1)==None):\n",
    "                    ret[key1] = {}\n",
    "                ret[key1][key2] = tmp_dict[key1][key2]\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "# divide the dataset in train/test set\n",
    "np.random.seed(10)\n",
    "folds = divide_dataset(user_based_utility_matrix)\n",
    "\n",
    "train_dict = merge_dicts([fold for fold in folds[:3]])\n",
    "test_dict = folds[3] \n",
    "\n",
    "#try to clear memory usage\n",
    "del user_based_utility_matrix\n",
    "del folds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used in computing similarity measure between two users \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Calculates the average value of the \n",
    "    valeues in a\n",
    "\n",
    "    a -> dictionary of non zero values \n",
    "\"\"\"\n",
    "def avg(a):\n",
    "    i=0\n",
    "    tot = 0\n",
    "    for k in a.keys():\n",
    "        i +=1\n",
    "        tot += a.get(k)\n",
    "    \n",
    "    return tot/i\n",
    "\n",
    "\"\"\"\n",
    "    Calculates a new dictionary that for each\n",
    "    non null value of a has that values - const\n",
    "\n",
    "    returns a - const\n",
    "\n",
    "    a -> dictioary of non zero values\n",
    "\"\"\"\n",
    "def scale(a,const):\n",
    "    ret = {}\n",
    "    \n",
    "    for k in a.keys():\n",
    "        ret[k] = a.get(k) - const\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\"\"\"\n",
    "    calculates the norm of a dictionary\n",
    "\n",
    "    a -> dictioary of non zero values\n",
    "\"\"\"\n",
    "def norm (a):\n",
    "    tot = 0\n",
    "    for k in a.keys():\n",
    "        tot += pow(a.get(k),2) \n",
    "\n",
    "    return math.sqrt(tot)\n",
    "\n",
    "\"\"\"\n",
    "    computes the inner product of two dictionaries\n",
    "\n",
    "    a -> dictioary of non zero values\n",
    "    b -> dictioary of non zero values\n",
    "\n",
    "\"\"\"\n",
    "def inner_product(a,b):\n",
    "    tot = 0\n",
    "    for k in a.keys():\n",
    "        b_tmp = b.get(k)\n",
    "        if(b_tmp != None):\n",
    "            tot += b_tmp * a.get(k) \n",
    "\n",
    "    return tot\n",
    "\n",
    "\"\"\"\n",
    "    Calculates the correlation coefficent\n",
    "        between two vectors\n",
    "    \n",
    "    a, b dictionaries of non zero values\n",
    "\n",
    "    esplicit -> boolean value that  defines if we\n",
    "    need to scale or not cause in the case of implicit\n",
    "    we do not need to scale\n",
    "\"\"\"\n",
    "def compute_correlation_coefficent(a,b):\n",
    "\n",
    "    avg_a = avg(a)\n",
    "    avg_b = avg(b)\n",
    "\n",
    "    a_scaled = scale(a, avg_a)\n",
    "    b_scaled = scale(b, avg_b)\n",
    "\n",
    "    a_scaled_norm = norm(a_scaled)\n",
    "    b_scaled_norm = norm(b_scaled)\n",
    "\n",
    "    if(a_scaled_norm == 0 or b_scaled_norm == 0):\n",
    "        # print('One of the two vectors has 0 norm, returning 0.')\n",
    "        return 0 \n",
    "\n",
    "    sim = inner_product(a_scaled,b_scaled)/(a_scaled_norm*b_scaled_norm)\n",
    "\n",
    "    return sim\n",
    "\n",
    "def compute_similarities(user_ids, utility_matrix, user_dict):\n",
    "    similarities = []\n",
    "    for user in user_ids:\n",
    "        tmp_user_dict = utility_matrix[user]\n",
    "        similarity = compute_correlation_coefficent(user_dict, tmp_user_dict)\n",
    "        similarities.append([user,similarity])\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "# definition of threads that will compute similarities for cliques generation\n",
    "class ComputeCorrelationCoefficentThread (threading.Thread):\n",
    "   \"\"\"\n",
    "      user_dict -> dictionary of the user: {itemID:rating}\n",
    "      user_ids -> set of users IDs\n",
    "      utility_matrix -> dataset\n",
    "   \"\"\"\n",
    "   def __init__(self, user_dict, user_ids, utility_matrix, name = None):\n",
    "      threading.Thread.__init__(self)\n",
    "      self.name = name\n",
    "      self.user_dict = user_dict\n",
    "      self.user_ids = user_ids\n",
    "      self.utility_matrix = utility_matrix\n",
    "      self.result = None\n",
    "   \n",
    "   def run(self):\n",
    "      if self.name == None:\n",
    "         raise Exception('Something bad happened, thread has no name')\n",
    "      #print('Thread {} started'.format(self.name))\n",
    "      self.result = compute_similarities(self.user_ids, self.utility_matrix, self.user_dict)\n",
    "      \n",
    "\n",
    "   # vector of similarities --> [userID, sim_score]\n",
    "   def join(self):\n",
    "        threading.Thread.join(self)\n",
    "        if self.result is not None:\n",
    "            return self.result\n",
    "        else:\n",
    "            print('Error using threads, result is None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition a list of unique items (data) into num_folds sublists\n",
    "\n",
    "def make_partitions(num_folds, data):\n",
    "    data_size = len(data)\n",
    "    fold_size = data_size // num_folds\n",
    "    partitions = []\n",
    "    for idx in range(num_folds-1):\n",
    "        partitions.append(data[idx*fold_size:(idx+1)*fold_size])\n",
    "    partitions.append(data[idx*fold_size:])\n",
    "    return partitions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Returns the clique of the user as np.array\n",
    "\n",
    "    user -> userID\n",
    "    utility_matrix -> {user:{item:rating}}\n",
    "    clique_size -> size of the clique of the user\n",
    "\"\"\"\n",
    "def compute_clique(user, utility_matrix, clique_size):\n",
    "\n",
    "    if type(cliques.get(user)) is np.ndarray:\n",
    "        print('\\nclique already computed, use existing one')\n",
    "        return cliques[user]\n",
    "\n",
    "    unique_users = list(set(utility_matrix.keys()))\n",
    "\n",
    "    # remove the user itself from the users to compare with\n",
    "    unique_users.remove(user)\n",
    "    user_dict = utility_matrix[user]\n",
    "    \n",
    "    \n",
    "    partitions = make_partitions(CORES,unique_users)        \n",
    "\n",
    "    threads = []\n",
    "    for idx in range(CORES):\n",
    "        name = \"Thread-{}\".format(idx)\n",
    "        thread = ComputeCorrelationCoefficentThread(name = name, \n",
    "                                                user_dict = user_dict, \n",
    "                                                user_ids = partitions[idx], \n",
    "                                                utility_matrix = utility_matrix)\n",
    "        threads.append(thread)\n",
    "\n",
    "    \n",
    "    [thread.start() for thread in threads]    \n",
    "    print('\\n======= {} threads started ======='.format(CORES))\n",
    "\n",
    "    results = [thread.join() for thread in threads]\n",
    "    print('\\n======= all threads joined =======')\n",
    "    \n",
    "    similarities = []\n",
    "    for elem in results:\n",
    "        for similarity in elem:\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "\n",
    "    similarities = np.array(similarities)\n",
    "    clique = similarities[np.argsort(similarities[:,1])[::-1]][:clique_size]\n",
    "\n",
    "    cliques[user] = clique\n",
    "\n",
    "    return clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    returns the predicted rating given a user and an item\n",
    "\n",
    "    user -> userID\n",
    "    item -> itemID\n",
    "    clique -> list of users that are similar to userID\n",
    "\"\"\"\n",
    "\n",
    "def predict(user, item, clique, utility_matrix):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for elem in clique:\n",
    "        neighbor = elem[0]\n",
    "        similarity = elem[1]\n",
    "\n",
    "        neigh_dict = utility_matrix[neighbor]\n",
    "        rating = neigh_dict.get(item)\n",
    "        \n",
    "        if  rating == None:\n",
    "            continue\n",
    "\n",
    "        numerator += rating*similarity\n",
    "        denominator += similarity\n",
    "    \n",
    "    if denominator == 0:\n",
    "        #print('denominator is 0, no user in the clique rated this item')\n",
    "        return -1\n",
    "    \n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# randomly select a subset of k users in test_dict for which we will score the recommender system \n",
    "# test users must have at least 15 ratings to test (so that, after removing the ones that we cannot predict (if no user in the clique rated the same item) we should have 10 ratings to rank)\n",
    "\n",
    "def select_test_users(train_dict, test_dict, movies_metadata, test_size, min_ratings_per_user):\n",
    "    np.random.seed(7)\n",
    "\n",
    "    selected_test_users = []\n",
    "\n",
    "    all_test_users = list(test_dict.keys())\n",
    "    print('\\nThe number of users in test set is {}'.format(len(all_test_users), flush=True))\n",
    "    print('\\nWe are taking into consideration only users with more than {} ratings'.format(min_ratings_per_user))\n",
    "    print('\\nThe subset of the test set taken into consideration is of size {}'.format(test_size))\n",
    "\n",
    "    all_train_users = list(train_dict.keys())\n",
    "\n",
    "    i=1\n",
    "    while i<=test_size:\n",
    "        #print('i is {}'.format(i), flush=True)\n",
    "        idx = np.random.randint(len(all_test_users))\n",
    "\n",
    "        user = all_test_users[idx]\n",
    "        if user not in all_train_users:\n",
    "            print(\"User is not in train set, can't compute clique\")\n",
    "            continue\n",
    "\n",
    "        user_items = list(test_dict[user].keys())\n",
    "        for item in user_items:\n",
    "            if movies_metadata.get(item) == None:\n",
    "                print(\"user has an item with no metadata, can't diversify with it\")\n",
    "                continue\n",
    "\n",
    "        # check if user has at least 15 ratings\n",
    "        num_user_ratings = len(list(test_dict[user].keys()))\n",
    "        if num_user_ratings < min_ratings_per_user:\n",
    "            #print('user has {} ratings'.format(num_user_ratings), flush=True)\n",
    "            continue      \n",
    "\n",
    "        # check if we already considered this user\n",
    "        if user in selected_test_users:\n",
    "            #print('user already considered', flush=True)\n",
    "            continue\n",
    "\n",
    "        # add user to list of selected users\n",
    "        selected_test_users.append(user)\n",
    "        i+=1\n",
    "\n",
    "    return selected_test_users\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nThe number of users in test set is 232351\n\nWe are taking into consideration only users with more than 30 ratings\n\nThe subset of the test set taken into consideration is of size 100\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[221093,\n 62448,\n 79554,\n 109497,\n 135635,\n 143955,\n 265660,\n 218583,\n 267230,\n 202638,\n 216529,\n 46210,\n 226298,\n 228606,\n 137233,\n 169004,\n 140345,\n 60227,\n 143341,\n 24561,\n 256238,\n 217266,\n 129628,\n 90360,\n 219349,\n 268392,\n 215252,\n 59826,\n 21908,\n 26108,\n 204871,\n 170185,\n 23275,\n 223988,\n 176898,\n 2751,\n 226587,\n 2645,\n 260048,\n 77064,\n 235436,\n 32198,\n 36489,\n 242604,\n 174028,\n 169300,\n 88160,\n 115184,\n 138889,\n 210485,\n 14009,\n 209279,\n 264313,\n 37187,\n 219367,\n 266783,\n 198723,\n 189363,\n 173996,\n 149719,\n 56258,\n 147857,\n 94532,\n 203536,\n 69304,\n 191542,\n 249350,\n 167818,\n 39632,\n 219141,\n 200020,\n 39246,\n 23811,\n 56687,\n 13710,\n 123037,\n 158085,\n 174001,\n 2203,\n 261933,\n 29855,\n 99052,\n 766,\n 97318,\n 187351,\n 152593,\n 157150,\n 235018,\n 195456,\n 168246,\n 250906,\n 100327,\n 69549,\n 168388,\n 215768,\n 152170,\n 166280,\n 262458,\n 74649,\n 31591]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "selected = select_test_users(train_dict, test_dict, movies_metadata, 100, 30)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list value is computed by taking the true rating of each element returned multyplied with a factor that depends on the position at which the element is in the ranked recommendation predicted\n",
    "\n",
    "def compute_list_value(predicted_ranking, true_ranking):\n",
    "    list_value = 0\n",
    "    for index, row in predicted_ranking.iterrows():\n",
    "        rating = true_ranking[true_ranking['item'] == row['item']]['true_rating'].values[0]\n",
    "        item_score = (1/(np.log(index+np.e)))*rating \n",
    "        list_value += item_score\n",
    "\n",
    "        # print(index, rating)\n",
    "    return list_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of jacccard coefficient score and dissimilarity measure\n",
    "\n",
    "\n",
    "default_similarity = 0.1\n",
    "def jaccard_score(list_1, list_2):\n",
    "    intersection = []\n",
    "    union = []\n",
    "    for elem in list_1:\n",
    "        if elem not in intersection and elem in list_2:\n",
    "            intersection.append(elem)\n",
    "        union.append(elem)\n",
    "    for elem in list_2:\n",
    "        if elem not in union:\n",
    "            union.append(elem)\n",
    "\n",
    "\n",
    "    try:\n",
    "        score = len(intersection)/len(union)\n",
    "    except:\n",
    "        print('division by zero, assuming a similarity of {}'.format(default_similarity))\n",
    "        return default_similarity\n",
    "    return score\n",
    "\n",
    "def dissimilarity_measure(id_1, id_2, movies_metadata):\n",
    "    metadata_1 = movies_metadata[id_1]\n",
    "    metadata_2 = movies_metadata[id_2]\n",
    "\n",
    "    #spoken_language_1 = metadata_1['spoken_language']\n",
    "    #spoken_language_2 = metadata_2['spoken_language']\n",
    "\n",
    "    original_language_1 = [metadata_1['original_language']]\n",
    "    original_language_2 = [metadata_2['original_language']]\n",
    "\n",
    "    production_companies_1 = metadata_1['production_companies']\n",
    "    production_companies_2 = metadata_2['production_companies']\n",
    "\n",
    "    production_countries_1 = metadata_1['production_countries']\n",
    "    production_countries_2 = metadata_2['production_countries']\n",
    "\n",
    "    genres_1 = metadata_1['genres']\n",
    "    genres_2 = metadata_2['genres']\n",
    "\n",
    "    #spoken_language_score = jaccard_score(spoken_language_1, spoken_language_2)\n",
    "    original_language_score = jaccard_score(original_language_1, original_language_2)\n",
    "    production_companies_score = jaccard_score(production_companies_1, production_companies_2)\n",
    "    production_countries_score = jaccard_score(production_countries_1, production_countries_2)\n",
    "    genres_score = jaccard_score(genres_1, genres_2)    \n",
    "\n",
    "    scores = np.array([original_language_score, production_companies_score, production_countries_score, genres_score])\n",
    "\n",
    "    weights = np.array([0.15, 0.15, 0.1, 0.6])\n",
    "\n",
    "    similarity = np.dot(weights, scores)\n",
    "    return 1-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_list_similarity(l,movies_metadata):\n",
    "    similarity = 0\n",
    "    n_items = len(l)\n",
    "\n",
    "    for i in range(n_items - 1):\n",
    "        for j in range(i + 1, n_items):\n",
    "            (item1, item2) = (l[i], l[j])\n",
    "            similarity += 1-dissimilarity_measure(item1, item2, movies_metadata)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiation_algorithm(old_list, movies_metadata,diversification_factor):\n",
    "    new_list = old_list[:1] # The first element remains always the same\n",
    "    old_ranking = [list((rank, old_list[rank - 1], 0)) for rank in range(1, len(old_list) + 1)] # List of triples (rank(starting from 1), item, dissimilarity_actual_value)\n",
    "    new_item = new_list[0]\n",
    "    \n",
    "\n",
    "    for i in range(1, 10):\n",
    "        old_ranking = list(filter(lambda x: x[1] != new_item, old_ranking)) # Remove the element passed in the previous iteration to the new list from the old ranking\n",
    "        dissimilarities = []\n",
    "        \n",
    "        for index in range(len(old_ranking)):\n",
    "            triple = old_ranking[index]\n",
    "            old_ranking[index][2] += dissimilarity_measure(int(triple[1]), int(new_item), movies_metadata) # Update the dissimilarity value considering the last added item (avg)\n",
    "            dissimilarities.append(old_ranking[index])\n",
    "        \n",
    "        dissimilarities.sort(reverse=True, key=lambda tup: tup[2]) # Sort in decreasing order according to the dissimilarity value\n",
    "        dissimilarity_rank = 0\n",
    "        min_rank = len(old_list) + 1 + len(dissimilarities)\n",
    "        \n",
    "        for triple in dissimilarities:\n",
    "            new_rank = triple[0] * (1 - diversification_factor) + dissimilarity_rank * diversification_factor # We consider the rank for both the old list and the dissimilarity sorted list\n",
    "            if new_rank < min_rank:\n",
    "                new_item = triple[1]\n",
    "                min_rank = new_rank\n",
    "            dissimilarity_rank += 1\n",
    "        \n",
    "        new_list.append(new_item)\n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select optimal ranking, predicted ranking, and optimal top 10, then score the result using list_value metric\n",
    "def score_predictions(predictions, movies_metadata, diversification_factor, k):\n",
    "    optimal_ranking = predictions[['item', 'true_rating']].sort_values(by=['true_rating'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    predicted = predictions[['item', 'predicted_rating']].sort_values(by=['predicted_rating'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if predicted.shape[0] > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    assert(predicted.shape[0] <= 50, 'predicted still contains more than 50 elements, something wring happened')    \n",
    "\n",
    "    if diversification_factor != 0:\n",
    "        # print('diversification factor != 0, start differentiating')\n",
    "        predicted_top_10 = differentiation_algorithm(list(predicted['item'].values), movies_metadata, diversification_factor)\n",
    "        mask = [elem in predicted_top_10 for elem in predicted['item'].values]\n",
    "        predicted_top_10 = predicted[mask].reset_index(drop=True)\n",
    "    else:\n",
    "        predicted_top_10 = predicted[:10]\n",
    "    optimal_top_10 = optimal_ranking[:10]\n",
    "\n",
    "    # compute similarity score of the original list and the diversified list\n",
    "    sim_original = intra_list_similarity(list(predicted['item'][:10].values), movies_metadata)\n",
    "    sim_diversified = intra_list_similarity(list(predicted_top_10['item'].values), movies_metadata)\n",
    "\n",
    "    # compute list value both for the optimal ranking and the predicted one\n",
    "    list_value = compute_list_value(predicted_top_10, optimal_ranking)\n",
    "    best_list_value = compute_list_value(optimal_top_10, optimal_ranking)\n",
    "    \n",
    "    # the metric is MAE, considering as error the difference of the list values of the optimal top 10 list and the predicted one\n",
    "    difference = best_list_value - list_value\n",
    "    print('Optimal ranking value: {}\\tpredicted ranking value: {}\\t difference: {}'.format(best_list_value, list_value,difference))\n",
    "\n",
    "    return difference, sim_original, sim_diversified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_test(train_dict, test_dict, movies_metadata, clique_size = 50, test_size = 5, min_ratings_per_user = 20, differentiation_factor = 0, k=50):\n",
    "\n",
    "    print('\\nClique size is {}'.format(clique_size))\n",
    "\n",
    "    selected_test_users = select_test_users(train_dict, test_dict, movies_metadata, test_size, min_ratings_per_user)\n",
    "    assert(len(selected_test_users) == test_size, \"Sizes don't match: something is wrong with selecting test users!\")\n",
    "\n",
    "    MAE = 0\n",
    "    mean_original_similarity = 0\n",
    "    mean_diversified_similarity = 0\n",
    "    for user in selected_test_users:\n",
    "        \n",
    "        test_items = list(test_dict[user].keys())\n",
    "        clique = compute_clique(user, train_dict, clique_size)\n",
    "\n",
    "\n",
    "        predictions_and_truth = pd.DataFrame( np.array( [[item, predict(user, item, clique, train_dict), test_dict[user][item]] for item in test_items] ),columns=['item','predicted_rating','true_rating'] )\n",
    "        predictions_and_truth = predictions_and_truth[predictions_and_truth.predicted_rating != -1]\n",
    "\n",
    "        difference, sim_original, sim_diversified = score_predictions(predictions_and_truth, movies_metadata, differentiation_factor,k)\n",
    "        assert(difference >= 0, 'Error: list value greater than best list value')\n",
    "        \n",
    "        MAE += difference\n",
    "        mean_original_similarity += sim_original\n",
    "        mean_diversified_similarity += sim_diversified\n",
    "        \n",
    "        print('\\nintra-list similarity score of original list is {}, while the diversified list scored {}'.format(sim_original, sim_diversified))\n",
    "        \n",
    "\n",
    "    MAE = MAE/test_size\n",
    "    mean_original_similarity = mean_original_similarity/test_size\n",
    "    mean_diversified_similarity = mean_diversified_similarity/test_size\n",
    "    \n",
    "    print('\\n======= TEST TERMINATED ======= ')\n",
    "    print('Mean absolute error is {}\\nMean intra-similarity score for non-diversified predictions is {}\\nMean intra-similarity score for diversified predictions is {}\\nResults have been computed on a subset of the test dictionary of {} users'.format(MAE, mean_original_similarity, mean_diversified_similarity, test_size ))\n",
    "\n",
    "    return MAE, mean_original_similarity, mean_diversified_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y score of original list is 12.504166666666668, while the diversified list scored 8.74416666666667\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 26.50186885876826\t difference: 2.1146597595239705\n\nintra-list similarity score of original list is 12.273095238095234, while the diversified list scored 9.068333333333335\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 22.061726786618546\t difference: 6.554801831673682\n\nintra-list similarity score of original list is 9.597500000000002, while the diversified list scored 4.16\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 26.480443631885354\t difference: 2.1360849864068747\n\nintra-list similarity score of original list is 18.031130952380945, while the diversified list scored 10.47\n\nclique already computed, use existing one\nOptimal ranking value: 27.50712949524161\tpredicted ranking value: 22.215046407102545\t difference: 5.292083088139066\n\nintra-list similarity score of original list is 12.015714285714285, while the diversified list scored 6.683333333333335\n\nclique already computed, use existing one\nOptimal ranking value: 28.210214525196783\tpredicted ranking value: 23.0583963382877\t difference: 5.151818186909082\n\nintra-list similarity score of original list is 16.487500000000004, while the diversified list scored 9.516428571428573\n\nclique already computed, use existing one\nOptimal ranking value: 27.244639215733365\tpredicted ranking value: 23.024286362751404\t difference: 4.220352852981961\n\nintra-list similarity score of original list is 16.29833333333333, while the diversified list scored 9.84642857142857\n\nclique already computed, use existing one\nOptimal ranking value: 26.610809207999168\tpredicted ranking value: 20.39811046223199\t difference: 6.212698745767177\n\nintra-list similarity score of original list is 12.572500000000003, while the diversified list scored 8.454464285714288\n\nclique already computed, use existing one\nOptimal ranking value: 27.09317550207445\tpredicted ranking value: 24.602835011319677\t difference: 2.4903404907547717\n\nintra-list similarity score of original list is 13.696190476190473, while the diversified list scored 11.02904761904762\n\nclique already computed, use existing one\nOptimal ranking value: 24.59623458071449\tpredicted ranking value: 20.751753213674295\t difference: 3.8444813670401956\n\nintra-list similarity score of original list is 13.100000000000001, while the diversified list scored 11.245000000000006\n\nclique already computed, use existing one\ndivision by zero, assuming a similarity of 0.1\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 17.49786569694658\t difference: 11.11866292134565\n\nintra-list similarity score of original list is 17.022499999999997, while the diversified list scored 8.345000000000004\n\nclique already computed, use existing one\nOptimal ranking value: 26.39773037219099\tpredicted ranking value: 18.543117231711012\t difference: 7.854613140479977\n\nintra-list similarity score of original list is 15.548571428571426, while the diversified list scored 9.018743842364534\n\nclique already computed, use existing one\ndivision by zero, assuming a similarity of 0.1\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.134411605145623\t difference: 3.482117013146606\n\nintra-list similarity score of original list is 10.94875, while the diversified list scored 7.61154761904762\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 22.079565326946597\t difference: 6.536963291345632\n\nintra-list similarity score of original list is 16.7310119047619, while the diversified list scored 10.658333333333333\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 24.41791273271624\t difference: 4.19861588557599\n\nintra-list similarity score of original list is 16.005, while the diversified list scored 12.094999999999997\n\nclique already computed, use existing one\nOptimal ranking value: 28.413371571744506\tpredicted ranking value: 25.67229314213611\t difference: 2.7410784296083968\n\nintra-list similarity score of original list is 13.288095238095236, while the diversified list scored 5.768750000000001\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 23.642484424622186\t difference: 4.974044193670043\n\nintra-list similarity score of original list is 13.384345238095237, while the diversified list scored 6.6072023809523825\n\nclique already computed, use existing one\nOptimal ranking value: 25.8727498131745\tpredicted ranking value: 20.15462975171087\t difference: 5.71812006146363\n\nintra-list similarity score of original list is 14.592142857142848, while the diversified list scored 10.772142857142862\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 23.09068649364875\t difference: 5.525842124643479\n\nintra-list similarity score of original list is 17.31732142857143, while the diversified list scored 7.59809523809524\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.308658784522382\t difference: 3.3078698337698462\n\nintra-list similarity score of original list is 9.671964285714285, while the diversified list scored 5.554583333333334\n\nclique already computed, use existing one\nOptimal ranking value: 25.754875756463008\tpredicted ranking value: 22.063031753149854\t difference: 3.691844003313154\n\nintra-list similarity score of original list is 13.415714285714284, while the diversified list scored 6.397619047619049\n\nclique already computed, use existing one\nOptimal ranking value: 27.244639215733365\tpredicted ranking value: 24.560423696666117\t difference: 2.684215519067248\n\nintra-list similarity score of original list is 13.745833333333334, while the diversified list scored 10.5325\n\nclique already computed, use existing one\nOptimal ranking value: 26.39773037219099\tpredicted ranking value: 24.469078059292475\t difference: 1.9286523128985138\n\nintra-list similarity score of original list is 18.099999999999998, while the diversified list scored 14.030000000000001\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 20.025584342279153\t difference: 8.590944276013076\n\nintra-list similarity score of original list is 17.75458333333333, while the diversified list scored 11.969999999999995\n\nclique already computed, use existing one\nOptimal ranking value: 27.788620631957908\tpredicted ranking value: 20.73053953424499\t difference: 7.058081097712918\n\nintra-list similarity score of original list is 13.724350649350649, while the diversified list scored 5.33309523809524\n\nclique already computed, use existing one\nOptimal ranking value: 26.754730395995992\tpredicted ranking value: 17.24519599771635\t difference: 9.50953439827964\n\nintra-list similarity score of original list is 15.745714285714282, while the diversified list scored 5.2183333333333355\n\nclique already computed, use existing one\ndivision by zero, assuming a similarity of 0.1\ndivision by zero, assuming a similarity of 0.1\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 22.333600191035412\t difference: 6.282928427256817\n\nintra-list similarity score of original list is 17.631428571428568, while the diversified list scored 10.726666666666667\n\nclique already computed, use existing one\nOptimal ranking value: 28.20257462512507\tpredicted ranking value: 24.02976015090531\t difference: 4.172814474219759\n\nintra-list similarity score of original list is 13.299999999999999, while the diversified list scored 4.210714285714286\n\nclique already computed, use existing one\nOptimal ranking value: 28.413371571744506\tpredicted ranking value: 18.192457533947355\t difference: 10.22091403779715\n\nintra-list similarity score of original list is 15.774166666666666, while the diversified list scored 11.403333333333334\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.128488985329543\t difference: 3.4880396329626855\n\nintra-list similarity score of original list is 12.620833333333328, while the diversified list scored 8.165000000000003\n\nclique already computed, use existing one\nOptimal ranking value: 27.303972448693887\tpredicted ranking value: 24.03879850973065\t difference: 3.265173938963237\n\nintra-list similarity score of original list is 15.244166666666665, while the diversified list scored 6.3100000000000005\n\nclique already computed, use existing one\nOptimal ranking value: 28.413371571744506\tpredicted ranking value: 25.409580870111757\t difference: 3.0037907016327487\n\nintra-list similarity score of original list is 16.06238095238095, while the diversified list scored 10.51195512820513\n\nclique already computed, use existing one\nOptimal ranking value: 25.340921763295846\tpredicted ranking value: 23.82564765732992\t difference: 1.5152741059659256\n\nintra-list similarity score of original list is 9.530833333333335, while the diversified list scored 3.9404761904761902\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.468739277288133\t difference: 3.1477893410040956\n\nintra-list similarity score of original list is 13.953333333333328, while the diversified list scored 10.615000000000002\n\nclique already computed, use existing one\nOptimal ranking value: 25.340921763295846\tpredicted ranking value: 19.10715296459218\t difference: 6.2337687987036645\n\nintra-list similarity score of original list is 12.584999999999999, while the diversified list scored 8.812916666666668\n\nclique already computed, use existing one\nOptimal ranking value: 25.58599803998485\tpredicted ranking value: 16.606224409694722\t difference: 8.97977363029013\n\nintra-list similarity score of original list is 12.45952380952381, while the diversified list scored 5.688398268398268\n\nclique already computed, use existing one\nOptimal ranking value: 20.531570032804556\tpredicted ranking value: 18.424478416155274\t difference: 2.1070916166492815\n\nintra-list similarity score of original list is 14.907142857142858, while the diversified list scored 10.121666666666671\n\nclique already computed, use existing one\nOptimal ranking value: 28.20257462512507\tpredicted ranking value: 25.300180927735706\t difference: 2.902393697389364\n\nintra-list similarity score of original list is 14.027499999999996, while the diversified list scored 8.069166666666668\n\nclique already computed, use existing one\nOptimal ranking value: 25.526208063219716\tpredicted ranking value: 19.449801717507626\t difference: 6.0764063457120905\n\nintra-list similarity score of original list is 9.929166666666669, while the diversified list scored 7.020000000000003\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 23.783770091348263\t difference: 4.832758526943966\n\nintra-list similarity score of original list is 12.843333333333332, while the diversified list scored 6.8133333333333335\n\nclique already computed, use existing one\nOptimal ranking value: 27.751796754787872\tpredicted ranking value: 23.62066819623762\t difference: 4.131128558550252\n\nintra-list similarity score of original list is 14.312083333333334, while the diversified list scored 11.18404761904762\n\nclique already computed, use existing one\nOptimal ranking value: 25.8727498131745\tpredicted ranking value: 18.971589848891455\t difference: 6.901159964283046\n\nintra-list similarity score of original list is 13.645000000000001, while the diversified list scored 7.9600000000000035\n\nclique already computed, use existing one\nOptimal ranking value: 26.051718709915285\tpredicted ranking value: 20.52711659429737\t difference: 5.524602115617913\n\nintra-list similarity score of original list is 14.04083333333333, while the diversified list scored 9.315\n\nclique already computed, use existing one\nOptimal ranking value: 26.254875756463008\tpredicted ranking value: 22.59858072249635\t difference: 3.656295033966657\n\nintra-list similarity score of original list is 8.744583333333335, while the diversified list scored 6.795000000000002\n\nclique already computed, use existing one\nOptimal ranking value: 24.890143892958644\tpredicted ranking value: 20.542218321516504\t difference: 4.34792557144214\n\nintra-list similarity score of original list is 13.480714285714283, while the diversified list scored 9.428928571428573\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 22.65399903226449\t difference: 5.962529586027738\n\nintra-list similarity score of original list is 20.233636363636357, while the diversified list scored 13.363928571428572\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.11071752776705\t difference: 3.505811090525178\n\nintra-list similarity score of original list is 13.455833333333333, while the diversified list scored 11.19404761904762\n\nclique already computed, use existing one\nOptimal ranking value: 23.77395432444111\tpredicted ranking value: 17.85403195887957\t difference: 5.91992236556154\n\nintra-list similarity score of original list is 14.773690476190476, while the diversified list scored 10.180714285714284\n\nclique already computed, use existing one\nOptimal ranking value: 26.887064891283508\tpredicted ranking value: 20.54095961655053\t difference: 6.346105274732977\n\nintra-list similarity score of original list is 14.42, while the diversified list scored 11.243333333333334\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 22.71188474598914\t difference: 5.90464387230309\n\nintra-list similarity score of original list is 11.943095238095237, while the diversified list scored 5.537857142857143\n\nclique already computed, use existing one\nOptimal ranking value: 28.20257462512507\tpredicted ranking value: 27.230299511312413\t difference: 0.9722751138126569\n\nintra-list similarity score of original list is 14.415416666666673, while the diversified list scored 7.754761904761907\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.94439745212309\t difference: 2.6721311661691374\n\nintra-list similarity score of original list is 14.015, while the diversified list scored 9.172261904761905\n\nclique already computed, use existing one\nOptimal ranking value: 25.840921763295846\tpredicted ranking value: 20.920848661426266\t difference: 4.92007310186958\n\nintra-list similarity score of original list is 10.602976190476198, while the diversified list scored 7.740714285714287\n\nclique already computed, use existing one\nOptimal ranking value: 27.751796754787872\tpredicted ranking value: 22.403698024903903\t difference: 5.348098729883969\n\nintra-list similarity score of original list is 17.019523809523808, while the diversified list scored 12.306666666666668\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 24.579595794909256\t difference: 4.036932823382973\n\nintra-list similarity score of original list is 14.903333333333336, while the diversified list scored 4.068333333333333\n\nclique already computed, use existing one\nOptimal ranking value: 27.568744617390873\tpredicted ranking value: 21.77949675693587\t difference: 5.789247860455003\n\nintra-list similarity score of original list is 8.294047619047621, while the diversified list scored 5.045833333333334\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 25.57309066510017\t difference: 3.0434379531920577\n\nintra-list similarity score of original list is 15.4925, while the diversified list scored 7.263095238095241\n\nclique already computed, use existing one\nOptimal ranking value: 28.413371571744506\tpredicted ranking value: 26.160856104633783\t difference: 2.2525154671107224\n\nintra-list similarity score of original list is 17.322678571428575, while the diversified list scored 13.80827380952381\n\nclique already computed, use existing one\nOptimal ranking value: 27.244639215733365\tpredicted ranking value: 23.735800950437252\t difference: 3.508838265296113\n\nintra-list similarity score of original list is 11.770000000000003, while the diversified list scored 7.935000000000003\n\nclique already computed, use existing one\nOptimal ranking value: 27.50712949524161\tpredicted ranking value: 25.68162714380284\t difference: 1.8255023514387716\n\nintra-list similarity score of original list is 13.498636363636365, while the diversified list scored 8.510416666666668\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 17.413969345188693\t difference: 11.202559273103535\n\nintra-list similarity score of original list is 19.812380952380952, while the diversified list scored 12.574166666666667\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 27.231545487011743\t difference: 1.3849831312804852\n\nintra-list similarity score of original list is 17.266607142857143, while the diversified list scored 11.927142857142856\n\nclique already computed, use existing one\nOptimal ranking value: 25.754875756463008\tpredicted ranking value: 23.819073737048036\t difference: 1.9358020194149717\n\nintra-list similarity score of original list is 8.494561403508776, while the diversified list scored 5.909561403508773\n\nclique already computed, use existing one\nOptimal ranking value: 28.413371571744506\tpredicted ranking value: 22.50619782767381\t difference: 5.907173744070697\n\nintra-list similarity score of original list is 16.139999999999997, while the diversified list scored 5.327857142857144\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 26.70065779950882\t difference: 1.9158708187834073\n\nintra-list similarity score of original list is 14.280714285714284, while the diversified list scored 11.223333333333338\n\nclique already computed, use existing one\ndivision by zero, assuming a similarity of 0.1\ndivision by zero, assuming a similarity of 0.1\ndivision by zero, assuming a similarity of 0.1\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 26.329561742996844\t difference: 2.2869668752953842\n\nintra-list similarity score of original list is 11.027380952380955, while the diversified list scored 4.459880952380952\n\nclique already computed, use existing one\nOptimal ranking value: 23.77395432444111\tpredicted ranking value: 22.57057554477281\t difference: 1.2033787796682986\n\nintra-list similarity score of original list is 14.473214285714285, while the diversified list scored 5.897500000000001\n\nclique already computed, use existing one\nOptimal ranking value: 27.54863970824015\tpredicted ranking value: 24.540689839933055\t difference: 3.0079498683070938\n\nintra-list similarity score of original list is 14.60083333333333, while the diversified list scored 11.9925\n\nclique already computed, use existing one\nOptimal ranking value: 28.61652861829223\tpredicted ranking value: 20.414870486722787\t difference: 8.201658131569442\n\nintra-list similarity score of original list is 14.505952380952381, while the diversified list scored 9.640000000000004\n\nclique already computed, use existing one\nOptimal ranking value: 25.145476633412386\tpredicted ranking value: 21.34584462270054\t difference: 3.7996320107118464\n\nintra-list similarity score of original list is 14.260714285714284, while the diversified list scored 6.077380952380953\n\n======= TEST TERMINATED ======= \nMean absolute error is 4.699609212570728\nMean intra-similarity score for non-diversified predictions is 13.942539823991808\nMean intra-similarity score for diversified predictions is 8.41832756627825\nResults have been computed on a subset of the test dictionary of 100 users\nElapsed time is 9.209076881408691 seconds\n"
    }
   ],
   "source": [
    "diversification_factors = list(np.linspace(0,1,11))\n",
    "test_size = 100\n",
    "min_ratings_per_user = 30\n",
    "\n",
    "MAEs = {}\n",
    "mean_similarities_non_diversified = {}\n",
    "mean_similarities_diversified = {}\n",
    "for diversification_factor in diversification_factors:\n",
    "    print('start testing with diversification factor equal to {}'.format(diversification_factor))\n",
    "    start = time.time()\n",
    "    \n",
    "    MAE, mean_similarity_non_diversified, mean_similarity_diversified = perform_test(train_dict, \n",
    "                                                                                test_dict,  \n",
    "                                                                                movies_metadata, \n",
    "                                                                                clique_size=200, \n",
    "                                                                                test_size = test_size, \n",
    "                                                                                min_ratings_per_user=min_ratings_per_user,\n",
    "                                                                                differentiation_factor = diversification_factor)\n",
    "    MAEs[diversification_factor] = MAE\n",
    "    mean_similarities_non_diversified[diversification_factor] = mean_similarity_non_diversified\n",
    "    mean_similarities_diversified[diversification_factor] = mean_similarity_diversified\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Elapsed time is {} seconds'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n======= MEAN ABSOLUTE ERRORS =======\n{0.0: 3.446476440437875, 0.1: 3.4517728680047064, 0.8: 4.13024432850261, 0.30000000000000004: 3.552592210329985, 1.0: 4.699609212570728, 0.9: 4.326145019295419, 0.2: 3.507908905859683, 0.6000000000000001: 3.831236425829357, 0.7000000000000001: 3.9802063748153116, 0.4: 3.6733510154310296, 0.5: 3.7973734534853385}\n\n======= MEAN SIMILARITIES NON DIVERSIFIED =======\n{0.0: 13.942539823991808, 0.1: 13.942539823991808, 0.8: 13.942539823991808, 0.30000000000000004: 13.942539823991808, 1.0: 13.942539823991808, 0.9: 13.942539823991808, 0.2: 13.942539823991808, 0.6000000000000001: 13.942539823991808, 0.7000000000000001: 13.942539823991808, 0.4: 13.942539823991808, 0.5: 13.942539823991808}\n\n======= MEAN SIMILARITIES DIVERSIFIED =======\n{0.0: 13.942539823991808, 0.1: 13.524888633515614, 0.8: 9.091561861676338, 0.30000000000000004: 11.772095684797655, 1.0: 8.41832756627825, 0.9: 8.678380604045081, 0.2: 12.719396425723398, 0.6000000000000001: 10.042431757058731, 0.7000000000000001: 9.587541623581098, 0.4: 11.029860657325125, 0.5: 10.453371134348107}\n"
    }
   ],
   "source": [
    "print('\\n======= MEAN ABSOLUTE ERRORS =======')\n",
    "print(MAEs)\n",
    "\n",
    "print('\\n======= MEAN SIMILARITIES NON DIVERSIFIED =======')\n",
    "print(mean_similarities_non_diversified)\n",
    "\n",
    "print('\\n======= MEAN SIMILARITIES DIVERSIFIED =======')\n",
    "print(mean_similarities_diversified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_path = os.path.join(os.getcwd(),'scores')\n",
    "user_based_scores_path = os.path.join(scores_path, 'user_based')\n",
    "\n",
    "MAEs_path = os.path.join(user_based_scores_path, 'MAEs.txt')\n",
    "sim_non_diversified_path = os.path.join(user_based_scores_path, 'intra-similarities_scores_non_diversified.txt')\n",
    "sim_diversified_path = os.path.join(user_based_scores_path, 'intra-similarities_scores_diversified.txt')\n",
    "\n",
    "with open(MAEs_path,'w') as fw:\n",
    "    json.dump(MAEs, fw)\n",
    "\n",
    "with open(sim_non_diversified_path,'w') as fw:\n",
    "    json.dump(mean_similarities_non_diversified, fw)\n",
    "\n",
    "with open(sim_diversified_path,'w') as fw:\n",
    "    json.dump(mean_similarities_diversified, fw)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595006259709",
   "display_name": "Python 3.5.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}